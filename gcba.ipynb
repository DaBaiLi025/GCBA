{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention_weights = nn.Linear(self.hidden_dim * 2, 1)  # 乘以2因为是双向\n",
    "\n",
    "    def forward(self, gru_output):\n",
    "        attention_scores = self.attention_weights(gru_output).squeeze(2)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(2)\n",
    "        weighted_output = gru_output * attention_weights\n",
    "        context_vector = weighted_output.sum(dim=1)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class CNN_GRUModel(nn.Module):\n",
    "    ",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, cnn_out_channels, cnn_kernel_size, pool_kernel_size, dropout_prob, fc_layers=[]):\n",
    "        super(CNN_GRUModel, self).__init__()\n",
    "        \n",
    "        # CNN层\n",
    "        self.cnn = nn.Conv1d(in_channels=input_dim, out_channels=cnn_out_channels, kernel_size=cnn_kernel_size, stride=1, padding=cnn_kernel_size // 2)\n",
    "        \n",
    "        # 池化层\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_kernel_size)\n",
    "\n",
    "        # Batch Normalization层\n",
    "        self.batch_norm = nn.BatchNorm1d(cnn_out_channels)\n",
    "\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # 双向GRU层\n",
    "        self.gru = nn.GRU(input_size=cnn_out_channels,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout_prob if num_layers > 1 else 0,\n",
    "                          bidirectional=True)\n",
    "        # 注意力层\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "        # 更新全连接层序列\n",
    "        fc_sequence = [nn.Linear(2 * hidden_dim if i == 0 else fc_layers[i-1], layer_size) for i, layer_size in enumerate(fc_layers)]\n",
    "        fc_sequence.append(nn.Linear(fc_layers[-1] if fc_layers else 2 * hidden_dim, output_dim))\n",
    "        self.fc_layers = nn.Sequential(*fc_sequence)\n",
    "\n",
    "    # forward函数保持不变\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 调整维度适应CNN层\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # CNN层\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        # ReLU激活函数\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 调整维度适应双向GRU层\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        # 应用注意力层\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # 全连接层\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 模型参数\n",
    "cnn_out_channels = 256  # 卷积层输出的通道数\n",
    "cnn_kernel_size = 3    # 卷积层的核大小\n",
    "dropout_prob = 0.2     # Dropout层的概率\n",
    "input_dim = len(features_columns)\n",
    "hidden_dim = 512\n",
    "output_dim = 1\n",
    "pool_kernel_size = 2\n",
    "num_layers = 3  # 增加GRU层的数量\n",
    "fc_layers = [256, 256] # 添加的全连接层的单元数列表\n",
    "\n",
    "",
    "lr=0.0001\n",
    "num_epochs = 200\n",
    "# 实例化模型\n",
    "model = CNN_GRUModel(input_dim=input_dim, hidden_dim=hidden_dim,output_dim=output_dim,num_layers=num_layers,cnn_out_channels=cnn_out_channels,cnn_kernel_size=cnn_kernel_size,pool_kernel_size=pool_kernel_size,dropout_prob=dropout_prob,fc_layers=fc_layers)\n",
    "# 定义损失函数\n",
    "criterion = nn.HuberLoss()\n",
    "# 将模型转移到GPU上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# 训练模型的函数\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    best_rmse = np.inf\n",
    "    best_r2 = -np.inf\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 计算平均训练损失\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}')\n",
    "        \n",
    "        # 在每个epoch后评估模型\n",
    "        rmse, r2 = evaluate_model(model, test_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test RMSE: {rmse:.4f}, Test R^2: {r2:.4f}')\n",
    "        \n",
    "        # 更新最佳指标和epoch\n",
    "        if rmse < best_rmse or (rmse == best_rmse and r2 > best_r2):\n",
    "            best_rmse = rmse\n",
    "            best_r2 = r2\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "    print(f'Best Test RMSE: {best_rmse:.4f} at epoch {best_epoch}')\n",
    "    print(f'Best Test R^2: {best_r2:.4f} at epoch {best_epoch}')\n",
    "    return best_epoch, best_rmse, best_r2\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():  # 在评估模式下不跟踪梯度\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.view(-1).tolist())\n",
    "            actuals.extend(target.view(-1).tolist())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    return rmse, r2\n",
    "\n",
    "\n",
    "# 开始训练并在每个epoch后评估\n",
    "best_epoch, best_rmse, best_r2 = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GOA(train_func, num_grasshoppers, num_iterations, param_space):\n",
    "    lb = np.array([param_space[key][0] for key in param_space])\n",
    "    ub = np.array([param_space[key][1] for key in param_space])\n",
    "\n",
    "    dim = len(param_space)\n",
    "    grasshoppers = [Grasshopper(np.random.uniform(lb, ub, dim)) for _ in range(num_grasshoppers)]\n",
    "\n",
    "    best_grasshopper = Grasshopper(np.zeros(dim))\n",
    "    best_cost = float('inf')\n",
    "\n",
    "    c_max = 1\n",
    "    c_min = 0.00001\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        c = c_max - iteration * ((c_max - c_min) / num_iterations)\n",
    "\n",
    "        for i in range(num_grasshoppers):\n",
    "            new_position = np.zeros(dim)\n",
    "            for j in range(num_grasshoppers):\n",
    "                if i != j:\n",
    "                    distance = np.linalg.norm(grasshoppers[i].position - grasshoppers[j].position)\n",
    "                    s = (ub - lb) * (0.5 * np.cos(distance) + 0.5)\n",
    "                    new_position += c * s * (grasshoppers[j].position - grasshoppers[i].position)\n",
    "\n",
    "            grasshoppers[i].position = np.clip(new_position, lb, ub)\n",
    "            cost = train_func(grasshoppers[i].position)\n",
    "\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_grasshopper.position = np.copy(grasshoppers[i].position)\n",
    "\n",
    "        print(f\"Iteration {iteration}: Best Cost = {best_cost}\")\n",
    "\n",
    "    return best_grasshopper.position\n",
    "\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    # Unpack parameters\n",
    "    hidden_dim, num_layers, cnn_out_channels, dropout_prob, lr = params\n",
    "    \n",
    "    # Assumed other necessary parameters - adjust these based on your model\n",
    "    input_dim = len(features_columns)  # Appropriate input dimension\n",
    "    output_dim = 1                     # Appropriate output dimension\n",
    "    cnn_kernel_size = 3                # Kernel size for CNN layers\n",
    "    pool_kernel_size = 2               # Kernel size for pooling layers\n",
    "    fc_layers = [256, 256]             # Full connected layers parameters, if any\n",
    "\n",
    "    # Instantiate the model and set hyperparameters\n",
    "    model = CNN_GRUModel(input_dim=input_dim, \n",
    "                         hidden_dim=int(hidden_dim), \n",
    "                         output_dim=output_dim, \n",
    "                         num_layers=int(num_layers), \n",
    "                         cnn_out_channels=int(cnn_out_channels), \n",
    "                         cnn_kernel_size=cnn_kernel_size, \n",
    "                         pool_kernel_size=pool_kernel_size, \n",
    "                         dropout_prob=dropout_prob,\n",
    "                         fc_layers=fc_layers)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to appropriate device (CPU or GPU)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch  # Adjust these based on your DataLoader structure\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Evaluate the model and return loss or other metric\n",
    "    return loss  # or another metric\n",
    "\n",
    "\n",
    "# 定义超参数搜索空间\n",
    "param_space = {\n",
    "    'hidden_dim': [32, 512],\n",
    "    'num_layers': [1, 4],\n",
    "    'cnn_out_channels': [16, 256],\n",
    "    'dropout_prob': [0.1, 0.5],\n",
    "    'lr': [0.0001, 0.001]\n",
    "}\n",
    "\n",
    "# 运行蝗虫优化算法\n",
    "best_params = GOA(train_and_evaluate, num_grasshoppers=30, num_iterations=100, param_space=param_space)\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
